{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###**Generative Adversarial Networks <br>**\n",
        "\n",
        "(Riya Arora)\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9pTKgi4T8t4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZG6LwCfY_vj",
        "outputId": "6b6679c4-9954-45b4-f9e3-184249d264e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import os\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "nT-JpBiF-xic"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhwdT6J94o_V",
        "outputId": "da641c25-6841-48de-98d7-abfee74a070a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### **Part A: Implement standard GAN on MNIST data.**"
      ],
      "metadata": {
        "id": "Rcyc426l9Qgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Preparation**"
      ],
      "metadata": {
        "id": "cZLma_qr-63O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root=\"./data\", train=False, transform=transforms.ToTensor(), download=True\n",
        ")"
      ],
      "metadata": {
        "id": "p0RpN5j5BUhl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e027047-e882-43ed-927a-750124e9ef53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.06MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 65.8kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 245kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.64MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 1 Batch is like\n",
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IObis2tyBdZS",
        "outputId": "8149292b-9a2c-4f4c-820a-fad47809e2ee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Set-Up**"
      ],
      "metadata": {
        "id": "qIxaS1wo1yOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Using only fully connected layers for now\n",
        "        # Hoping for results because MNIST dataset should be simple enough\n",
        "        # Will try later with linear + deconvolutional layers\n",
        "        self.fc1 = nn.Linear(z_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, 1024)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.fc4 = nn.Linear(1024, 28*28)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = torch.relu(self.bn1(self.fc1(z)))\n",
        "        z = torch.relu(self.bn2(self.fc2(z)))\n",
        "        z = torch.relu(self.bn3(self.fc3(z)))\n",
        "        z = self.fc4(z)\n",
        "        z = z.view(z.size(0), 1, 28, 28)\n",
        "        return self.tanh(z)"
      ],
      "metadata": {
        "id": "vwkE4JJcC0nC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Using only fully-connected layers for now\n",
        "        self.fc1 = nn.Linear(28*28, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = self.fc4(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7SDGQeLg7e_-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight initialization\n",
        "def xavier_weights_init(m):\n",
        "    if isinstance(m, nn. Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "T0e40Zc0KYfV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulating a run of input through the model (Not training yet)\n",
        "\n",
        "z_dim = 100\n",
        "batch_size = 64\n",
        "generator = Generator(z_dim)\n",
        "discriminator = Discriminator()\n",
        "\n",
        "generator.apply(xavier_weights_init)\n",
        "discriminator.apply(xavier_weights_init)\n",
        "\n",
        "random_noise_input = torch.randn(batch_size, z_dim)\n",
        "generated_images = generator(random_noise_input)\n",
        "generated_images = generated_images.view(generated_images.size(0), -1)\n",
        "\n",
        "real_images, _ = next(iter(train_loader))\n",
        "real_images = real_images.view(real_images.size(0), -1)\n",
        "\n",
        "real_output = discriminator(real_images)\n",
        "fake_output = discriminator(generated_images)\n",
        "\n",
        "print(real_output)\n",
        "print(fake_output)"
      ],
      "metadata": {
        "id": "p6RC8uwW9FqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f183589-ec23-4a85-a218-d9713b02557c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4890],\n",
            "        [0.5220],\n",
            "        [0.4803],\n",
            "        [0.5215],\n",
            "        [0.5541],\n",
            "        [0.5255],\n",
            "        [0.5715],\n",
            "        [0.4819],\n",
            "        [0.5318],\n",
            "        [0.5445],\n",
            "        [0.5103],\n",
            "        [0.5019],\n",
            "        [0.5331],\n",
            "        [0.5844],\n",
            "        [0.5473],\n",
            "        [0.5365],\n",
            "        [0.5453],\n",
            "        [0.6126],\n",
            "        [0.5601],\n",
            "        [0.5649],\n",
            "        [0.4705],\n",
            "        [0.5220],\n",
            "        [0.4873],\n",
            "        [0.5491],\n",
            "        [0.5012],\n",
            "        [0.5125],\n",
            "        [0.4858],\n",
            "        [0.5520],\n",
            "        [0.5335],\n",
            "        [0.4719],\n",
            "        [0.4686],\n",
            "        [0.5530],\n",
            "        [0.4709],\n",
            "        [0.5541],\n",
            "        [0.5103],\n",
            "        [0.5493],\n",
            "        [0.5518],\n",
            "        [0.5052],\n",
            "        [0.5095],\n",
            "        [0.4569],\n",
            "        [0.5292],\n",
            "        [0.5135],\n",
            "        [0.4784],\n",
            "        [0.5516],\n",
            "        [0.5123],\n",
            "        [0.4908],\n",
            "        [0.5297],\n",
            "        [0.4989],\n",
            "        [0.5263],\n",
            "        [0.5214],\n",
            "        [0.5207],\n",
            "        [0.4960],\n",
            "        [0.5250],\n",
            "        [0.5770],\n",
            "        [0.5534],\n",
            "        [0.4834],\n",
            "        [0.5308],\n",
            "        [0.5538],\n",
            "        [0.5107],\n",
            "        [0.4985],\n",
            "        [0.5489],\n",
            "        [0.5344],\n",
            "        [0.4837],\n",
            "        [0.5024]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[0.4215],\n",
            "        [0.6568],\n",
            "        [0.4288],\n",
            "        [0.4404],\n",
            "        [0.4946],\n",
            "        [0.5655],\n",
            "        [0.5565],\n",
            "        [0.4925],\n",
            "        [0.5549],\n",
            "        [0.4379],\n",
            "        [0.5718],\n",
            "        [0.5379],\n",
            "        [0.5410],\n",
            "        [0.4970],\n",
            "        [0.5204],\n",
            "        [0.6560],\n",
            "        [0.5749],\n",
            "        [0.5038],\n",
            "        [0.4739],\n",
            "        [0.6060],\n",
            "        [0.5357],\n",
            "        [0.5379],\n",
            "        [0.6218],\n",
            "        [0.4942],\n",
            "        [0.5123],\n",
            "        [0.5334],\n",
            "        [0.5952],\n",
            "        [0.6171],\n",
            "        [0.5529],\n",
            "        [0.5246],\n",
            "        [0.5618],\n",
            "        [0.5514],\n",
            "        [0.4685],\n",
            "        [0.5247],\n",
            "        [0.4728],\n",
            "        [0.5353],\n",
            "        [0.4756],\n",
            "        [0.6538],\n",
            "        [0.4107],\n",
            "        [0.5769],\n",
            "        [0.5212],\n",
            "        [0.4793],\n",
            "        [0.4727],\n",
            "        [0.5540],\n",
            "        [0.4976],\n",
            "        [0.4753],\n",
            "        [0.5212],\n",
            "        [0.5436],\n",
            "        [0.5182],\n",
            "        [0.4947],\n",
            "        [0.4738],\n",
            "        [0.5776],\n",
            "        [0.4831],\n",
            "        [0.6049],\n",
            "        [0.6296],\n",
            "        [0.5743],\n",
            "        [0.5304],\n",
            "        [0.5443],\n",
            "        [0.4418],\n",
            "        [0.4657],\n",
            "        [0.5548],\n",
            "        [0.4531],\n",
            "        [0.5084],\n",
            "        [0.4995]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introducing Loss Functions**"
      ],
      "metadata": {
        "id": "5A4fx49_Q_Mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Introducing generator loss function\n",
        "\n",
        "z_dim = 100\n",
        "batch_size = 64\n",
        "bce_loss = nn.BCELoss()\n",
        "real_label = 0.9 # Trying with smoothening\n",
        "fake_label = 0.0\n",
        "\n",
        "g1 = Generator(z_dim).to(device)\n",
        "g1.apply(xavier_weights_init)\n",
        "d1 = Discriminator().to(device)\n",
        "d1.apply(xavier_weights_init)\n",
        "\n",
        "real_images, _ = next(iter(train_loader))\n",
        "real_images = real_images.to(device).view(real_images.size(0), -1)\n",
        "real_output = d1(real_images)\n",
        "d1_real_loss = bce_loss(real_output, torch.full((batch_size, 1), real_label, device = device))\n",
        "\n",
        "random_noise_input = torch.randn(batch_size, z_dim, device = device)\n",
        "generated_images = g1(random_noise_input)\n",
        "generated_images = generated_images.view(generated_images.size(0), -1)\n",
        "fake_output = d1(generated_images.detach())\n",
        "d1_fake_loss = bce_loss(fake_output, torch.full((batch_size, 1), fake_label, device = device))\n",
        "\n",
        "d1_total_loss = d1_real_loss + d1_fake_loss"
      ],
      "metadata": {
        "id": "vZ8aN2X0_UIC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Introducing discriminator loss function\n",
        "\n",
        "fake_output = d1(generated_images)\n",
        "g1_total_loss = bce_loss(fake_output, torch.full((batch_size, 1), real_label, device = device))\n"
      ],
      "metadata": {
        "id": "yi-cUlKENcbC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAN Training**"
      ],
      "metadata": {
        "id": "SFHqghptRHKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the hyperparameters\n",
        "\n",
        "z_dim = 100\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "momentum = 0.5\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "ptIz7iIYOSgs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Optimizers and Loss\n",
        "\n",
        "G = Generator(z_dim).to(device)\n",
        "D = Discriminator().to(device)\n",
        "G.apply(xavier_weights_init)\n",
        "D.apply(xavier_weights_init)\n",
        "\n",
        "G_optimizer = torch.optim.Adam(G.parameters(), lr=lr, betas=(momentum, 0.999))\n",
        "D_optimizer = torch.optim.Adam(D.parameters(), lr=lr, betas=(momentum, 0.999))\n",
        "\n",
        "bce_loss = nn.BCELoss()"
      ],
      "metadata": {
        "id": "62VfpheWRjwx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics through the epochs\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "D_accuracies = []"
      ],
      "metadata": {
        "id": "f6tcFjhEVaXR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and Loading Checkpoints\n",
        "\n",
        "import torch\n",
        "\n",
        "def save_checkpoint(epoch, g_model, d_model, g_optimizer, d_optimizer,checkpoint_path):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'g_model_state_dict': g_model.state_dict(),\n",
        "        'd_model_state_dict': d_model.state_dict(),\n",
        "        'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
        "        'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} to {checkpoint_path}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_path, g_model, d_model, g_optimizer, d_optimizer):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    g_model.load_state_dict(checkpoint['g_model_state_dict'])\n",
        "    d_model.load_state_dict(checkpoint['d_model_state_dict'])\n",
        "    g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "    d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "    print(f\"Checkpoint loaded from epoch {epoch} from {checkpoint_path}\")\n",
        "    return epoch"
      ],
      "metadata": {
        "id": "-RdZ_cBHW53E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth'\n",
        "for epoch in range(epochs):\n",
        "    for real_images, _ in train_loader:\n",
        "        real_images = real_images.to(device).view(real_images.size(0), -1)\n",
        "\n",
        "        real_labels = torch.full((real_images.size(0), 1), 0.9, device=device)\n",
        "        fake_labels = torch.zeros(real_images.size(0), 1, device=device)\n",
        "\n",
        "        # Generator part\n",
        "        real_output = D(real_images)\n",
        "        D_real_loss = bce_loss(real_output, real_labels)\n",
        "\n",
        "        random_noise = torch.randn(real_images.size(0), z_dim, device=device)\n",
        "        generated_images = G(random_noise)\n",
        "        fake_output = D(generated_images.detach())\n",
        "        D_fake_loss = bce_loss(fake_output, fake_labels)\n",
        "        D_total_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "        D_optimizer.zero_grad()\n",
        "        D_total_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        real_pred = (real_output > 0.5).float()\n",
        "        fake_pred = (fake_output < 0.5).float()\n",
        "        D_accuracy = (real_pred.sum() + fake_pred.sum()) / (2 * real_images.size(0))\n",
        "\n",
        "        D_losses.append(D_total_loss.item())\n",
        "        D_accuracies.append(D_accuracy.item())\n",
        "\n",
        "        # Generator part\n",
        "        fake_labels.fill_(0.9)\n",
        "        fake_output = D(generated_images)\n",
        "        G_loss = bce_loss(fake_output, fake_labels)\n",
        "\n",
        "        G_optimizer.zero_grad()\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        G_losses.append(G_loss.item())\n",
        "\n",
        "    if (epoch % 10 == 0 or epoch == (epochs-1)):\n",
        "            save_checkpoint(epoch, G, D, G_optimizer, D_optimizer, checkpoint_path)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}]  D Loss: {D_total_loss.item():.4f}  G Loss: {G_loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HZlT3OzSTj2",
        "outputId": "4f63cca0-74a5-4fdd-b02d-178b3eedbd11"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch 0 to /content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth\n",
            "Epoch [1/50]  D Loss: 0.6283  G Loss: 4.1495\n",
            "Epoch [2/50]  D Loss: 0.4473  G Loss: 5.7977\n",
            "Epoch [3/50]  D Loss: 0.5119  G Loss: 6.6272\n",
            "Epoch [4/50]  D Loss: 0.6486  G Loss: 4.6833\n",
            "Epoch [5/50]  D Loss: 0.4388  G Loss: 5.5219\n",
            "Epoch [6/50]  D Loss: 0.6262  G Loss: 3.0481\n",
            "Epoch [7/50]  D Loss: 0.6435  G Loss: 4.4768\n",
            "Epoch [8/50]  D Loss: 0.5399  G Loss: 8.2189\n",
            "Epoch [9/50]  D Loss: 0.5735  G Loss: 8.1047\n",
            "Epoch [10/50]  D Loss: 0.5209  G Loss: 5.7884\n",
            "Checkpoint saved at epoch 10 to /content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth\n",
            "Epoch [11/50]  D Loss: 0.7038  G Loss: 6.9928\n",
            "Epoch [12/50]  D Loss: 0.5890  G Loss: 3.3204\n",
            "Epoch [13/50]  D Loss: 0.4904  G Loss: 6.6650\n",
            "Epoch [14/50]  D Loss: 0.5411  G Loss: 4.9197\n",
            "Epoch [15/50]  D Loss: 0.5010  G Loss: 4.4561\n",
            "Epoch [16/50]  D Loss: 0.6221  G Loss: 6.4596\n",
            "Epoch [17/50]  D Loss: 0.7377  G Loss: 5.4798\n",
            "Epoch [18/50]  D Loss: 0.7804  G Loss: 7.7102\n",
            "Epoch [19/50]  D Loss: 0.5463  G Loss: 4.8547\n",
            "Epoch [20/50]  D Loss: 0.6919  G Loss: 3.8380\n",
            "Checkpoint saved at epoch 20 to /content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth\n",
            "Epoch [21/50]  D Loss: 0.7996  G Loss: 3.0473\n",
            "Epoch [22/50]  D Loss: 0.4875  G Loss: 5.0397\n",
            "Epoch [23/50]  D Loss: 0.6998  G Loss: 3.8810\n",
            "Epoch [24/50]  D Loss: 0.8204  G Loss: 2.9448\n",
            "Epoch [25/50]  D Loss: 0.6878  G Loss: 4.6625\n",
            "Epoch [26/50]  D Loss: 0.7152  G Loss: 5.0779\n",
            "Epoch [27/50]  D Loss: 0.6918  G Loss: 4.3598\n",
            "Epoch [28/50]  D Loss: 0.7940  G Loss: 2.2905\n",
            "Epoch [29/50]  D Loss: 0.7790  G Loss: 3.4645\n",
            "Epoch [30/50]  D Loss: 0.7718  G Loss: 2.3841\n",
            "Checkpoint saved at epoch 30 to /content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth\n",
            "Epoch [31/50]  D Loss: 0.7201  G Loss: 3.0793\n",
            "Epoch [32/50]  D Loss: 0.7556  G Loss: 3.2248\n",
            "Epoch [33/50]  D Loss: 0.7443  G Loss: 3.6159\n",
            "Epoch [34/50]  D Loss: 0.6510  G Loss: 3.2073\n",
            "Epoch [35/50]  D Loss: 0.8123  G Loss: 4.0392\n",
            "Epoch [36/50]  D Loss: 0.8161  G Loss: 2.5931\n",
            "Epoch [37/50]  D Loss: 0.9596  G Loss: 3.7566\n",
            "Epoch [38/50]  D Loss: 0.8392  G Loss: 2.5775\n",
            "Epoch [39/50]  D Loss: 0.9588  G Loss: 4.4020\n",
            "Epoch [40/50]  D Loss: 0.7783  G Loss: 3.8805\n",
            "Checkpoint saved at epoch 40 to /content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth\n",
            "Epoch [41/50]  D Loss: 0.9259  G Loss: 3.4672\n",
            "Epoch [42/50]  D Loss: 0.8494  G Loss: 2.3913\n",
            "Epoch [43/50]  D Loss: 0.5491  G Loss: 3.8524\n",
            "Epoch [44/50]  D Loss: 0.6901  G Loss: 3.2674\n",
            "Epoch [45/50]  D Loss: 0.5630  G Loss: 4.2779\n",
            "Epoch [46/50]  D Loss: 0.7795  G Loss: 2.4364\n",
            "Epoch [47/50]  D Loss: 0.9616  G Loss: 1.6988\n",
            "Epoch [48/50]  D Loss: 0.7462  G Loss: 2.9010\n",
            "Epoch [49/50]  D Loss: 0.9526  G Loss: 2.2818\n",
            "Checkpoint saved at epoch 49 to /content/drive/MyDrive/gen-ai/gan-results/gan_ckpt.pth\n",
            "Epoch [50/50]  D Loss: 0.8052  G Loss: 2.6932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving metrics to disk\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"g_losses\": G_losses,\n",
        "    \"d_losses\": D_losses,\n",
        "    \"d_accuracies\": D_accuracies\n",
        "})\n",
        "\n",
        "metrics_save_path = '/content/drive/MyDrive/gen-ai/gan-results/metrics.csv'\n",
        "metrics_df.to_csv(metrics_save_path, index=False)"
      ],
      "metadata": {
        "id": "vNApoW8MU5km"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading metrics from disk\n",
        "\n",
        "metrics_read_path = '/content/drive/MyDrive/gen-ai/gan-results/metrics.csv'\n",
        "metrics_df = pd.read_csv(metrics_read_path)\n",
        "\n",
        "G_losses = metrics_df[\"g_losses\"].tolist()\n",
        "D_losses = metrics_df[\"d_losses\"].tolist()\n",
        "D_accuracies = metrics_df[\"d_accuracies\"].tolist()\n",
        "print(len(D_losses))"
      ],
      "metadata": {
        "id": "MLdqtxE0WaBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7972e2c-d945-4adb-9384-05141c9a13c1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aOdp_5Qaopx",
        "outputId": "b56144ac-8180-4236-f263-16e275c47d6b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_size = 938\n",
        "\n",
        "average_loss = []\n",
        "for i in range(0, len(D_accuracies), group_size):\n",
        "    chunk = D_accuracies[i:i + group_size]\n",
        "    chunk_average = sum(chunk) / len(chunk)\n",
        "    average_loss.append(chunk_average)\n",
        "\n",
        "print(average_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HkoM0obb9JO",
        "outputId": "a2d5c5a4-39b1-4fef-929f-b2d4c178c8ac"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9029850746268657, 0.9549323694029851, 0.9385244536247335, 0.9323027718550106, 0.9373084355010661, 0.9325193230277186, 0.9298707356076759, 0.9306536513859275, 0.9306203358208955, 0.9299207089552238, 0.9306786380597015, 0.927913446162047, 0.9252898454157783, 0.9237989738805971, 0.9216001465884861, 0.917952092217484, 0.9150786247334755, 0.9112723214285714, 0.9077242137526652, 0.9052671908315565, 0.9006946295309168, 0.8962719882729211, 0.8940481743070362, 0.8891008128997868, 0.884694829424307, 0.884694829424307, 0.8797391391257996, 0.8760660980810234, 0.8737173507462687, 0.8692947094882729, 0.8655800239872068, 0.8624150453091685, 0.8588252931769723, 0.8568430170575693, 0.8529867404051172, 0.8542610607675906, 0.8505380463752665, 0.8489139125799574, 0.8487639925373134, 0.8471565165245203, 0.8463236273987207, 0.8451159381663113, 0.8448077691897654, 0.8447661247334755, 0.8432252798507462, 0.8441997601279317, 0.8432002931769723, 0.8434668176972282, 0.8439582222814499, 0.8450493070362474]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXSYS5kWfaeB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}